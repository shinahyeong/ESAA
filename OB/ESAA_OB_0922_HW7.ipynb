{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Ch.08 텍스트 분석\n",
        "\n",
        "(1) NLP와 텍스트 분석의 차이\n",
        "- NLP: 머신이 인간의 언어를 이해하고 해석하는데 중점을 둠 -> 텍스트 분석을 향상하게 하는 기반 기술이라고 보면 됨\n",
        "- 텍스트 분석: 비정형 텍스트에서 의미 있는 정보를 추출하는 것에 중점을 둠\n",
        "  - 머신러닝, 언어 이해, 통계 등을 활용하여 모델을 수립하고 정보를 추출하여 비즈니스 인텔리전스나 예측 분석 등의 분석 작업을 주로 수행함\n",
        "  - 텍스트 부류, 감성분석, 텍스트 요약, 텍스트 군집화와 같은 작업 수행 가능"
      ],
      "metadata": {
        "id": "dWQkFKQLXYq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###텍스트 분석 이해\n",
        "\n",
        "(1) 텍스트 분석\n",
        "- 비정형 데이터인 텍스트를 분석하는 것\n",
        "- 머신러닝 알고리즘은 숫자형의 피처 기반 데이터만 입력받을 수 있기 때문에 텍스트를 머신러닝에 적용하기 위해서는 반드시 데이터를 피처 형태로 추출하여 추출된 피처에 의미있는 값을 찾아내는 과정이 필요함\n",
        "  - 피처 백터화: 텍스트를 word 기반의 다추의 피처로 추출한 뒤, 이 피처에 단어 빈도수와 같은 숫자값을 부여하여 벡터값으로 표현하는 과정  \n"
      ],
      "metadata": {
        "id": "WlmYpcHjYtM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**텍스트 분석 수행 프로세스**\n",
        "- 텍스트 사전 준비 작업(텍스트 전처리): 텍스트를 피처로 만들기 전 미리 클렌징, 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어 등의 토큰화 작업 등 정규화 작업을 수행하는 것을 통칭\n",
        "- 피처 벡터화/추출: 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 여기에 벡테값을 할당하는 방법으로 대표적으로 BOW가 있음\n",
        "- ML 모델 수립 및 학습/예측/평가: 피처 벡터화된 데이터 세트에 ML모델을 적용하여 학습/예측 및 평가를 수행"
      ],
      "metadata": {
        "id": "niK57bTcZV-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**파이썬 기반의 NLP, 텍스트 분석 패키지**\n",
        "- 대표적 패키지는 3가지가 존재\n",
        "  - NLTK: 가장 대표적인 NLP 패키지\n",
        "  - Gensim: 토픽 모델링 분야에서 강점을 가짐\n",
        "  - SpaCy: 뛰어난 수행성능을 가짐\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "dUSEjtguZ1L9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###텍스트 사전 준비작업(텍스트 전처리) - 텍스트 정규화"
      ],
      "metadata": {
        "id": "iYVgbZH0aLyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트 자체를 바로 피처로 만들기는 어려움\n",
        "  - 따라서 이를 사전에 텍스트를 가공하는 준비 작업이 필요함\n",
        "    - 클렌징\n",
        "    - 토큰화\n",
        "    - 필터링/스톱 워드 제거/철자 수정\n",
        "    - Stemming\n",
        "    - Lemmatization\n",
        "    "
      ],
      "metadata": {
        "id": "evtKSDuXaSoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**클렌징**\n",
        "- 텍스트에서 분석에 오히려 방해되는 불필요한 문자, 기호 등을 사전에 제거하는 작업\n",
        "\n",
        "**텍스트 토큰화**\n",
        "- 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화로 나눠짐\n",
        "- NLTK의 다양한 API를 통해 사용 가능\n",
        "- 문장 토큰화\n",
        "  - 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리\n",
        "  - 정규 표현식에 따른 문장 토큰화도 가능함"
      ],
      "metadata": {
        "id": "ACZOquo7af4M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUfsKAbPWm44",
        "outputId": "8a1ae5fa-04f2-4a88-ac00-cf1a8e6c84d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "               You can see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences),len(sentences))\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - 여기서 sent_tokenize()는 각각의 문장으로 구성된 list 객체를 반환함\n",
        "- 단어 토큰화\n",
        "  - 문장을 단어로 토큰화 하는 방법\n",
        "  - 공백, 콤마, 마침표, 개행 문자 등으로 단어를 분리하나 정규 표현식을 이용해서도 다양한 유형으로 토큰화 수행가능\n",
        "  - word_tokenize()이용"
      ],
      "metadata": {
        "id": "2rQ-nD9nbEj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOdLtzi_bbr_",
        "outputId": "8a868a86-a743-4c9a-ca22-63aa8ca6a31e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "# 여러 개의 문장으로 된 입력 데이터를 문장 별로 단어 토큰화하게 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "  # 문장별로 분리 토큰\n",
        "  sentences = sent_tokenize(text)\n",
        "  # 분리된 문장별 단어 토큰화\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens\n",
        "\n",
        "# 여러 문장에 대해 문장별 단어 토큰화 수행.\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqFTe3JVbueG",
        "outputId": "ef482fdf-6173-4d53-ef85-d399be714b13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 문장을 단어별로 하나씩 토큰화하면 문맥적인 의미는 무시될 수밖에 없음\n",
        "  - 이를 조금이라도 해결하고자 n-gram을 도입하여 연속된 n개의 단어를 하나의 토큰화 단위로 분리해내기도 함"
      ],
      "metadata": {
        "id": "GLVxm5AucX4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**스톱 워드 제거**\n",
        "- 분석에 큰 의미가 없는 단어를 의미함\n",
        "- 단어에서 문법적인 특성으로 인해 빈번하게 텍스트에 나타나는 의미없는 단어들도 존재하므로 이들을 사전에 제거해야 중요한 단어를 인지가능\n",
        "- 언어별로 스톱 워드는 목록화 되어 있으므로 NLTK의 스톱 워드 목록을 받아서 확인한 후 사용하는 것이 좋음"
      ],
      "metadata": {
        "id": "OwUS9St7ciEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7-Up3jZc0xx",
        "outputId": "d4e3b31c-e609-486e-e4d2-bfacd10dcd2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('영어 stop words 개수:', len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq5a5pLmc7KE",
        "outputId": "b68c1a4d-e327-4afd-d2a9-2a9c1d3eec4d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 개수: 198\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이를 이용하여 stopword를 제거하여 분석을 위한 의미있는 단어 추출을 수행"
      ],
      "metadata": {
        "id": "fR6R4p2QdGut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "# 위의 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
        "for sentence in word_tokens:\n",
        "  filtered_words=[]\n",
        "  # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
        "  for word in sentence:\n",
        "    word = word.lower()\n",
        "    # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaFbn3PqdLx3",
        "outputId": "833be574-0671-4f00-db88-1ffa3e4c1559"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming과 Lemmatization\n",
        "- 두가지 모두 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 기법임\n",
        "- 일반적으로 lemmatization이 Stemming보다 의미론적으로 단어의 원형을 찾음\n",
        "  - 대신 더 오랜 시간을 필요로 함"
      ],
      "metadata": {
        "id": "s-tZ7eJJeFSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZEDq5Q8eF8u",
        "outputId": "53abde4a-250f-4299-821e-2da4dab2d1ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordnetLemmatizer을 이용해 Lemmatization 수행하기\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing'))"
      ],
      "metadata": {
        "id": "JLUmdjoZeFqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f2e42d-2e7d-40f7-df0e-448b9a25b8e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amusing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words - BOW\n",
        "- 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도값을 부여하여 피처값ㅇㄹ 추출하는 모델\n",
        "- BOW 모델의 장점\n",
        "  - 쉽고 빠른 구축\n",
        "- BOW 모델의 단점\n",
        "  - 문맥 의미 반영 부족: 단어의 순서를 고려하지 않아 문장 내에서 단어의 문맥적인 의미가 무시됨\n",
        "  - 희소 행렬 문제: BOW로 피처 벡터화를 수해하는 경우 희소 행렬 형태의 데이터 세트가 만들어져, 수행시관과 예측 성능을 떨어트릴 수 있음"
      ],
      "metadata": {
        "id": "1gMHcBm7GLM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BOW 피처 벡터화**\n",
        "- 피처 벡터화: 텍스트를 특정 의미를 가지는 숫자형 값인 벡터값으로 변환하는 과정\n",
        "- BOW에서는 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 문서에서 해당하는 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 과정을 나타냄\n",
        "- BOW의 피처 벡터화 방식\n",
        "  - 카운트 기반 벡터화: 단어 피처에 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수를 부여하는 경우\n",
        "  - TF-IDF: 개별 문서에서 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서 페널티를 주는 방식\n",
        "    - 문서의 개수가 많거나 텍스트가 긴 경우, 카운트 방식보다 TF-IDF방식이 유리\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "U9yeXb89G1CH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**사이킷런의 Count 및 TF-IDF 벡터화 구현: CountVectorizer, TfidfVectorizer\n",
        "- CountVectorizer: 카운트 기반 벡터화를 구현한 클래스\n",
        "  - 피처벡터화만 수행하는 것이 아닌 소문자 일괄 변환, 토큰화 등의 텍스트 전처리도 함께 수행해줌\n",
        "  - 입력 파라미터\n",
        "    - max_df:지나치게 높은 빈도수를 가지는 단어피처를 제외하기 위한 파라미터\n",
        "    - min_df: 지나치게 낮은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터\n",
        "    - max_features: 추출하는 피처의 개수를 제한하며 정수로 값을 지정"
      ],
      "metadata": {
        "id": "wlaQqMr2Hi9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**희소행렬 - COO 형식**\n",
        "- COO 형식: 0이 아닌 데이터만 별도의 데이터 배열에 저장하고, 그 데이터가 가리키는 행과 위치를 별도의 배열로 저장하는 방식\n",
        "  - 파이썬에선 Scipy를 이용하여 희소행렬로 변환함\n",
        "  - coo_matrix클래스를 이용해 COO 형식의 희소행렬을 변환하고, 미리만든 행위치 배열과 열 위치 배열을 생성 파라미터로 입력"
      ],
      "metadata": {
        "id": "JZzpXoVYeAzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dense = np.array([[3,0,1], [0,2,0]])"
      ],
      "metadata": {
        "id": "_8w6vmSkeBFz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "#0이 아닌 데이터 추출\n",
        "data = np.array([3,1,2])\n",
        "\n",
        "#행 위치와 열 위치를 각각 배열로 생성\n",
        "row_pos = np.array([0,0,1])\n",
        "col_pos = np.array([0,2,1])\n",
        "\n",
        "# sparse 패키지의 coo_matrix를 이용해 COO형식으로 희소 행렬 생성\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
      ],
      "metadata": {
        "id": "zgm3RkRKeCA6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_coo.toarray()"
      ],
      "metadata": {
        "id": "ClphKUzueB5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e79acf2-d92d-4deb-d1ea-ffbc28f54e13"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**희소 행렬- CSR 형식**\n",
        "- CSR형식: COO형식의 행과 열의 위치를 나타내기 위한 반복적 위치 데이터 사용 문제를 해결한 형식\n",
        "  - 반복을 제거하는 위치의 위치를 표기하는 방식을 이용함\n",
        "  - COO 방식보다 메모리가 적게 들고 빠른 연산이 가능함\n",
        "  "
      ],
      "metadata": {
        "id": "YCsM4nmSwXzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "([0,0,1,0,0,5], [1,4,0,3,2,5], [0,6,0,3,0,0], [2,0,0,0,0,0], [0,0,0,7,0,8], [1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "spx96E17eBt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5233a95-1153-4f04-a0fd-aabd789a0a8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 0, 1, 0, 0, 5],\n",
              " [1, 4, 0, 3, 2, 5],\n",
              " [0, 6, 0, 3, 0, 0],\n",
              " [2, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 7, 0, 8],\n",
              " [1, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "dense2 = np.array([[0,0,1,0,0,5],\n",
        "                  [1,4,0,3,2,5],\n",
        "                  [0,6,0,3,0,0],\n",
        "                  [2,0,0,0,0,0],\n",
        "                  [0,0,0,7,0,8],\n",
        "                  [1,0,0,0,0,0]])\n",
        "\n",
        "#0이 아닌 데이터 추출\n",
        "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
        "\n",
        "#행 위치와 열 위치를 각각 array로 생성\n",
        "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
        "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
        "\n",
        "#COO 형식으로 변환\n",
        "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
        "\n",
        "#행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
        "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
        "\n",
        "#CSR 형식으로 변환\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "\n",
        "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_coo.toarray())\n",
        "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_csr.toarray())"
      ],
      "metadata": {
        "id": "p6jhjaJ7eBkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3855c162-b41b-468e-fd4c-20c7879d4292"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dense3 = np.array([[0,0,1,0,0,5],\n",
        "                  [1,4,0,3,2,5],\n",
        "                  [0,6,0,3,0,0],\n",
        "                  [2,0,0,0,0,0],\n",
        "                  [0,0,0,7,0,8],\n",
        "                  [1,0,0,0,0,0]])\n",
        "\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)"
      ],
      "metadata": {
        "id": "KiKikv78eBYV"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}