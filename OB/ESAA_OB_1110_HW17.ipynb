{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 4장. 딥러닝 시작\n",
        "##4.1 인공 신경망의 한계와 딥러닝의 출현\n",
        "- 인공신경망 구조의 역사 : 플랭크 로젠블라트가 1957에 고안한 퍼셉트론이라는 선형 분류기\n",
        "- 퍼셉트론: 다수의 신호를 입력으로 받아 하나의 신호를 출력하는데, 이 신호를 다시 입력으로 받아 흐른다/안흐른다(1/0)는 정보를 앞으로 전달하는 원리로 작용\n",
        "- AND 게이트: 모든 입력이 1일 때 작동\n",
        "  - 입력 중 어떤 하나라도 0을 갖는 다면 작동을 멈춤\n",
        "- OR 게이트: 입력에서 둘 중 하나만 1이거나 둘다 1일 때 작동\n",
        "  - 입력 모두가 0을 갖는 경우를 제외한 나머지가 모두 1 값을 갖게 됨\n",
        "- XOR 게이트: 배타적 논리합이라는 용어로 입력 두개 중 한개만 1일 때 작동하는 논리 연산\n",
        "  - 데이터가 비선형적으로 분리되기 때문에 제대로된 분류가 어려움\n",
        "  - 단층 퍼셉트론에서는 XOR 게이트 학습 불가능 (AND, OR 게이트만 학습)\n",
        "  \n",
        "  -> 이를 극복하는 방안: 입력층과 출력층 사이에 하나 이상의 중간층(은닉층)을 두어 비선형적으로 분리되는 데이터에 대해서도 학습이 가능하도록 다층 퍼셉트롭을 고안\n",
        "    - 이렇게 입력층과 출력층 사이에 은닉층이 여러 개 있는 신경망을 '심층 신경망'이라고 하며, 이는 다른 이름으로 '딥러닝'이라고 함"
      ],
      "metadata": {
        "id": "k-yPxyJ3pvsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2 딥러닝 구조\n",
        "- 딥러닝: 여러 층을 가진 인공 신경망을 사용하여 학습을 수행하는 것\n",
        "\n",
        "###4.2.1 딥러닝 용어\n",
        "- 딥러닝은 입력층, 출력층과 2개 이상의 은닉층으로 구성됨\n",
        "- 또한 입력 신호를 전달하기 위해 다양한 함수도 사용함\n",
        "  - 입력층: 데이터를 받아들이는 층\n",
        "  - 은닉층: 모든 입력 노드부터 입력 값을 받아 가중합을 계산하고, 이 값을 활성화함수에 적용하여 출력층에 전달하는 층\n",
        "  - 출력층: 신경망의 최종 결괏값이 포함된 충\n",
        "- 가중치: 입력 값이 연산 결과에 미치는 영향력을 조절하는 요소\n",
        "- 가중합 또는 전달 함수: 가중합은 전달 함수라도고 불리움\n",
        "  - 각 노드에서 신호에 가중치를 곱하여 다음 노드로 전달되는데, 이 값들을 모두 더한 합계를 가중합이라고 함\n",
        "  - 또한 노드의 가중합이 계산되면 이 가중합을 활성화 함수로 보내기 때문에 전달 함수라고도 불리움\n",
        "- 활성화 함수 : 전달 함수에서 전달받은 값을 출력할 때 일정 기준에 따라 출력 값을 변화시키는 비선형 함수\n",
        "  - 시그모이드, 하이퍼볼릭 탄젠트, 렐루 함수 등이 존재\n",
        "\n",
        "1) 시그모이드 함수\n",
        "- 선형 함수의 결과를 0~1사이에서 비선형 형태로 변형해주는 함수\n",
        "- 주로 로지스틱 회귀와 같은 분류 문제를 확률적으로 표현하는데 사용\n",
        "- 기울기 소멸 문제가 발생할 수 있다는 단점이 존재하여, 현재는 자주 사용되지 않음\n",
        "\n",
        "2) 하이퍼볼릭 탄젠트 함수\n",
        "- 선형 함수의 결과를 -1~1사이에서 비선형 형태로 변형해주는 함수\n",
        "- 시그모이드에서 결괏값의 평균이 0이 아닌 양수로 편향된 문제를 해결하는데 사용하나, 기울기 소멸 문제는 여전히 존재\n",
        "\n",
        "3) 레루 함수\n",
        "- 입력이 음수일 때는 0을, 양수일 때는 입력을 출력하는 함수\n",
        "- 경사 하강법에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않음\n",
        "- 일반적으로 은닉층에서 사용되며, 하이퍼볼릭 탄젠트 함수 대비 학습 속도가 6배 빠름\n",
        "- 음수 값을 받으면 항상 0을 출력하여 학습 능력이 감소함\n",
        "  - 이를 해결하고자 리키 렐루 함수를 이용\n",
        "\n",
        "4) 리키 렐루 함수\n",
        "- 입력 값이 음수이면 0이 아닌 0.001과 같은 매우 작은 수를 반환하여 입력값이 수렴하는 구간을 제거하여 렐루 함수의 문제를 해결하고자 함\n",
        "\n",
        "5) 소프트맥스 함수\n",
        "- 입력 값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 항상 1이 되도록 하는 함수\n",
        "- 딥러닝에서 출력 노드의 활성화 함수로 많이 이용"
      ],
      "metadata": {
        "id": "aSN0nd4PGFX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 렐루 함수와 소프트맥스 함수를 파이토치로 구현하기\n",
        "import torch\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, n_feature, n_hidden, n_output):\n",
        "    super(Net, self).__init__()\n",
        "    self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "    self.relu = torch.nn.ReLu(inplace=True)\n",
        "    self.out = torch.nn.Linear(n_hidden, n_output)\n",
        "    self.softmax = torch.nn.Softmax(dim=n_output)\n",
        "  def forward(self, x):\n",
        "    x = self.hidden(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.out(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "pSXwSfr-NpMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 손실 함수\n",
        "  - 경사하강법: 학습률과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트 하는 방법\n",
        "    - 즉, 미분의 기울기를 이용하여 오차를 비교하고 최소화하는 방향으로 이동시키는 방법\n",
        "    - 여기서 오차를 구하는 방법이 손실함수임\n",
        "  - 손실함수는 즉, 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표\n",
        "    - 값이 클수록 많이 틀렸다는 의미이며, 이값이 0에 가까우면 완벽하게 추정할수 있다는 의미임\n",
        "    - 대표적으로 평균 제곱 오차, 크로스 엔트로피 오차가 있음\n",
        "\n",
        "1) 평균 제곱 오차\n",
        "- 실제 값과 예측 값의 차이를 제곱하여 평균 낸 것\n",
        "- 실제 값과 예측 값의 차이가 클수록 평균 제곱 오차의 값도 커짐 -> 값이 작을수록 예측력이 좋아짐"
      ],
      "metadata": {
        "id": "h8QR7osSNqa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "# y_pred = model(x)\n",
        "# loss = loss_fn(y_pred, y)"
      ],
      "metadata": {
        "id": "INiISwkVN01l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) 크로스 엔트로피 오차\n",
        "- 분류 문제에서 원-핫 인코딩을 했을 때 사용할 수 있는 오차 계산법\n",
        "- 두 개의 확률 분포 차이를 이용하여 시그모이드 영향을덜 받기 때문에 학습 속도가 빠름"
      ],
      "metadata": {
        "id": "Ri-MGHm3N1Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = nn.CrossEntropyLoss()\n",
        "# input = torch.randn(5,6, requires_grad=True)\n",
        "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "# output = loss(input, target)\n",
        "# output.backward()"
      ],
      "metadata": {
        "id": "18O7D8g8N3TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2.2 딥러닝 학습\n",
        "- 딥러닝 학습은 크게 순전파와 역전파 2 단계로 진행됨\n",
        "- 순전파: 네트워크에 훈련 데이터가 들어올 때 발생함\n",
        "  - 데이터를 기반으로 예측 값을 계산하기 위해 전체 신경망을 교차하여 지나가게 됨\n",
        "  - 즉, 모든 뉴런이 이전 층의 뉴런에서 수신한 정보에 가중합 및 활성화 함수를 적용하여 다음층인 은닉층의 뉴런으로 전송하는 방식\n",
        "    - 네트워크를 통해 입력 데이터를 전달 -> 데이터가 모든 층을 통과하고 모든 뉴런이 계산 완료하면 예측값이 최종 층인 출력층에 도달\n",
        "- 역전파: 손실 함수 비용이 0에 가깝도록 하기 위해 모델이 훈련을 반복하면서 가중치를 조정해나가는데, 이 과정에서 손실이 계산되면 정보는 역으로 출력층에서 입력층으로 전달되는 현상이 나타남\n",
        "  - 이떼 출력층에서 시작된 손실 비용은 은닉층의 모든 뉴런으로 전파되나, 은닉층 뉴런은 각 뉴런이 원래 출력에 기여한 상대적 기여도에 따라 값이 달라짐"
      ],
      "metadata": {
        "id": "-UqBe_pdN3fW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.3 딥러닝의 문제점과 해결 방안\n",
        "- 딥러닝의 핵심: 활성화 함수가 적용된 여러 은닉층을 결합하여 비선형 영역을 표현하는 것\n",
        "- 은닉층이 많으면 생기는 문제점\n",
        "  - 과적합 문제\n",
        "    - 과적합: 훈련 데이터를 과하게 학습하여 발생\n",
        "    - 훈련 데이터에 대해 과하게 학습하여 실제 데이터에 대한 오차가 증가하는 현상이라고 생각하면 됨\n",
        "    - 과적합 해결 방법: 드롭 아웃\n",
        "      - 드롭 아웃: 신경망 모델이 과적합 되는 것을 피하기 위한 방법으로, 학습 과정 중 임의로 일부 노드들을 학습에서 제외시키는 방법"
      ],
      "metadata": {
        "id": "nx8RAwZsNO6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DropoutModel, self).__init__()\n",
        "    self.layer1 = torch.nn.Linear(784, 1200)\n",
        "    self.dropout1 = torch.nn.Dropout(0.5)\n",
        "    self.layer2 = torch.nn.Linear(1200,1200)\n",
        "    self.dropout2 = torch.nn.Dropout(0.5)\n",
        "    self.layer3 = torch.nn.Linear(1200,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = self.dropout1(x)\n",
        "    x = F.relu(self.layer2(x))\n",
        "    x = self.dropout2(x)\n",
        "    return self.layer3(x)"
      ],
      "metadata": {
        "id": "WZaTuVEONUbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 기울기 소멸 문제 발생\n",
        "  - 주로 은닉층이 많은 신경망에서 발생하는 문제로, 출력층에서 은닉층으로 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상을 나타냄\n",
        "  - 즉, 기울기가 소멸되기 때문에 학습되는 양이 0에 가까워져 학습이 더디게 진행되다 오차를 더 주이지 못하고 그 상태로 수렴하는 현상\n",
        "  - 기울기 소멸 문제 해결: 렐루 활성화 함수 사용\n",
        "- 성능이 나빠지는 문제 발생\n",
        "  - 경사 하강법은 손실 함수의 비용이 최소가 되는 지점을 찾을 때 까지 기울기가 낮은 쪽으로 계속 이동하는데, 이때 성능이 나빠지는 문제가 발생할 수 있음\n",
        "  - 해결 방법: 확률적 경사하강법, 미니 배치 경사 하강법 사용\n",
        "\n",
        "경사 하강법의 유형\n",
        "- 배치 경사 하강법: 전체 데이터 셋에 대한 오류를 구한 후 기울기를 한번만 계산하여 모델의 파라미터를 업데이트 하는 벙법\n",
        "  - 전체 훈련 데이터 셋에 대한 가중치를 편미분하는 방법\n",
        "  - 한 스텝에 모든 훈련 데이터셋을 사용하기 때문에 학습이 오래 걸리는 단점이 있음\n",
        "- 확률적 경사 하강법: 임의로 선택한 데이터에 대해 기울기를 계산하는 방법\n",
        "  - 적은 데이터를 사용하기 때문에 빠른 계산이 가능\n",
        "  - 배치 경사하강법보다 정확도가 낮고, 파라미터 변경 폭이 불안정하다는 단점 존재\n",
        "- 미니 배치 경사 하강법: 전체 데이터셋을 미니 배치 여러 개로 나누고, 미니 배치 한 개마다 기울기를 구한 후 그것의 평균 기울기를 이용해 모델을 업데이트하여 학습하는 방법\n",
        "  - 전체 데이터를 계산하는 것보다 빠르며, 확률적 경사 하강법보다 안정적임"
      ],
      "metadata": {
        "id": "CodEiy3DV7Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomDataset(Dataset):\n",
        "#   def __init(self):\n",
        "#     self.x.data = [[1,2,3], [4,5,6], [7,8,9]]\n",
        "#     self.y.eata = [[12],[18],[11]]\n",
        "#     def __len__(self):\n",
        "#       return len(self.x_data)\n",
        "#     def __getitem__(self, idx):\n",
        "#       x = torch.FloatTensor(self.x_data[idx])\n",
        "#       y = torch.FloatTensor(self.y_data[idx])\n",
        "#       return x, y\n",
        "# dataset = CustomDataset()\n",
        "# dataloader = DataLoader(\n",
        "#     dataset,\n",
        "#     batch_size=2,\n",
        "#     shuffle=True,\n",
        "# )"
      ],
      "metadata": {
        "id": "QpKURwdqXBhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2.4 딥러닝을 사용할 때 이점\n",
        "\n",
        "1. 특성 추출\n",
        "- 컴퓨터가 입력 받은 데이터를 분석하여 일정한 패턴이나 규칙을 찾아내기 위해선 사람이 인지하는 데이터를 컴퓨터가 인지할 수 있는 데이터로 변환해야 함\n",
        "- 딥러닝이 활성화 되기 이전엔 머신러닝 알고리즘 중 SVM, 나이브 베이즈, 로지스틱 회귀의 특성 추출을 이용\n",
        "  - 매우 복잡하고, 수집된 데이터에 대한 전문지식이 필요했음\n",
        "- 딥러닝이 활성화 된 후엔 특성 추출 과정이 알고리즘에 통합되어 데이터의 특성을 스스로 잘 잡아낼 수 있게됨\n",
        "\n",
        "2. 빅데이터의 효율적 활용\n",
        "- 딥러닝에서 특성 추출을 알고리즘에 통합 가능했던 이유가 바로 빅데이터 덕분임\n",
        "- 딥러닝 학습일 이용한 특성 추출은 데이터 사례가 많을 수록 성능이 향상되기 때문"
      ],
      "metadata": {
        "id": "xoOhdsNqNWMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.3. 딥러닝 알고리즘\n",
        "- 딥러닝 알고리즘은 공통적으로 심층 신경망을 사용함\n",
        "  - 목적에 따라 합성곱 신경망(CNN), 순환 신경망(RNN), 제한된 볼츠만 머신(RBM), 심층 신뢰 신경망(DBN)으로 분류"
      ],
      "metadata": {
        "id": "k3yXw0dgNaA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.3.1 심층 신경망\n",
        "- 입력층과 출력층 사이에 다수의 은닉층을 포함하는 인공 신경망\n",
        "- 다수의 은닉층을 추가했기 때문에 머신러닝에서의 비선형 분류에서 사용한 여러 트릭을 사용하지 않고도 비선형 분류가 가능\n",
        "- 다수의 은닉층을 두어 학습을 위한 연산량이 많고 기울기 소명 문제가 발생 가능\n",
        "  - 드롭 아웃, 렐루 함수, 배치 정규화 등을 적절히 수행하여 해결 가능"
      ],
      "metadata": {
        "id": "7Nhr769FNcfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.3.2 합성곱 신경망\n",
        "- 합성곱층과 풀링층을 포함하는 이미지 처리 성능이 좋은 인공 신경망 알고리즘\n",
        "- 영상 및 사진이 포함된 이미지 데이터에서 객체를 탐색하거나 객체 위치를 찾아내는 유용한 신경망임\n",
        "- 기존 신경망과 비교하여 차별성이 존재함\n",
        "  - 각층의 입출력 형상을 유지함\n",
        "  - 이미지 공간 정보를 유지하면서 인접 이미지와 차이가 있는 특징을 효과적으로 인식함\n",
        "  - 복수 필터로 이미지의 특징을 추출하고 학습함\n",
        "  - 추출한 이미지의 특징을 모으고 강화하는 풀링층이 존재함\n",
        "  - 필터를 공유 파라미터로 사용하기 때문에 일반 인공 신경망과 비교하여 학습 파라미터가 매우 적음\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "fYrll48oN9Fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.3.3 순환 신경망\n",
        "- 시계열 데이터와 같은 시간 흐름에 따라 변환하는 데이터를 학습하기 위한 인공 신경망\n",
        "- 여기서 순환은 자기 자신을 참조한다는 의미로, 현재 결과가 이전 결과와 연관이 있다는 의미로 쓰임\n",
        "- 순환 신경망 특징\n",
        "  - 시간성을 가진 데이터가 많음\n",
        "  - 시간성 정보를 이용하여 데이터의 특징을 잘 다룸\n",
        "  - 시간에 따라 내용이 변하므로 데이터는 동적이고, 길이가 가변적임\n",
        "  - 매우 긴 데이터를 처리하는 연구가 활발히 진행됨\n",
        "- 기울기 소명 문제로 학습이 제대로 되지 않는 경우가 있기 때문에, LSTM이라는 메모리 개념을 도입한 알고리즘이 많이 이용됨\n",
        "- 자연어 처리 분야와 잘 맞음"
      ],
      "metadata": {
        "id": "o0WAC1OYOApj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.3.4 제한된 볼츠만 머신\n",
        "- 가시층과 은닉층으로 구성된 모델\n",
        "  - 가시층: 은닉층과만 연결되기 때문에 제한된 볼츠만 머신이라 불리움\n",
        "- 제한된 볼츠만 머신 특징\n",
        "  - 차원 감소, 분류, 선형 회귀 분석, 협업 필터링, 특성 값 학습, 주제 모델링에 사용\n",
        "  - 기울기 소명 문제를 해결하기 위해 사전 학습 용도로 활용 가능\n",
        "  - 심층 신뢰 신경망의 요소로 활용됨\n",
        "- 많이 사용되는 알고리즘: CNN, RNN"
      ],
      "metadata": {
        "id": "dhJ5ZOGGODGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.3.5 심층 신뢰 신경망\n",
        "- 입력층과 은닉층으로 구성된 제한된 볼츠만 머신을 블록처럼 여러 층으로 쌓은 형태로 연결된 신경망\n",
        "- 레이블이 없는 데이터에 대한 비지도 학습이 가능함\n",
        "- 부분적인 이미지에서 전체를 연상하는 일반화와 추상화 과정을 구현할 때 사용하면 유용함\n",
        "- 학습 절차\n",
        "  - 가시층과 은닉층1에 제한된 볼츠만 머신을 사전 훈련함\n",
        "  - 첫번째 층 입력 데이터와 파라미터를 고정하여 두 번째 층 제한된 볼츠만 머신을 사전 훈련\n",
        "  - 원하는 층 개수만큼 제한된 볼츠만 머신을 쌓아 올려 전체 DBN을 완성\n",
        "- 심층 신뢰 신경망의 특징\n",
        "  - 순차적으로 심층 신뢰 신경망을 학습 시켜 가면서 계층적 구조 생성\n",
        "  - 비지도 학습으로 학습\n",
        "  - 위로 올라갈수록 추상적 특성을 추출\n",
        "  - 학습된 가중치를 다층 퍼셉트론으로 가중치 초깃값으로 사용"
      ],
      "metadata": {
        "id": "tFqUghgDOIad"
      }
    }
  ]
}