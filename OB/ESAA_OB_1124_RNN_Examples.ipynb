{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#7.3.3 문자 단위 RNN 실습 2가지\n",
        "- RNN의 입출력의 단위가 단어 레벨이 아닌 문자 레벨로 하여 RNN을 구현하면 이를 문자 단위 RNN이라고 함\n",
        "  - RNN의 구조 자체가 달리진 것이 아닌 입 출력의 단위가 문자로 바뀌었을 뿐임"
      ],
      "metadata": {
        "id": "ZOvrRK5IetBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###문자 단위 RNN(Char RNN)"
      ],
      "metadata": {
        "id": "N6M-S1KTe7fS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] 필요 도구 import"
      ],
      "metadata": {
        "id": "faxzVPiKe_AA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VulvH8mMem23"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] 훈련 데이터 전처리\n",
        "- apple을 입력 -> pple!룰 출력하도록 구현\n",
        "- 입력 데이터와 레이블 데이터에 대한 문자 집합을 만들도록 구현"
      ],
      "metadata": {
        "id": "jb_7Qt9ofCmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print('문자 집합의 크기: {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxY_sAW-fOSL",
        "outputId": "3fea93bc-52cc-4191-a001-db1d3cf53338"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size=vocab_size # 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "kDWi6JFTfOpm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c, in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_zXqfrpfQN5",
        "outputId": "afdb12bd-b2ef-4c35-d08a-e8eaca8c6c52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "  index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvoIHE2AfQV7",
        "outputId": "d02fa21c-9b96-4adc-8bc7-6f2f04635f1b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wcKRJCMfRDe",
        "outputId": "2cf74721-3ef4-4ea6-fb2f-434a8f62f1b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u3W1Uh0fR68",
        "outputId": "48cf3642-f51e-456c-85d7-c775d77e6e25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB7uCEQQfSMK",
        "outputId": "5bd4c8bf-ca92-417b-af22-2eda6a17b53e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRR_73IpfS2n",
        "outputId": "47781e90-1bbb-4ae4-8332-c0a961ccabb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2348034151.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기: {}'.format(X.shape))\n",
        "print('레이블의 크기: {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBbKLkbnfTM_",
        "outputId": "121d1aed-ec59-4059-ac28-9b0e73e2db3d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기: torch.Size([1, 5, 5])\n",
            "레이블의 크기: torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3] 모델 구현하기\n",
        "- fc는 완전 연결층을 의미하며, 출력층으로 사용됨"
      ],
      "metadata": {
        "id": "v_J_t36gfT01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)  # RNN  셀 구현\n",
        "    self.fc = torch.nn.Linear(hidden_size, output_size, bias=True)  # 출력층 구현\n",
        "\n",
        "  def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "PokxCcaffYWJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "Ity_U6Z8fYc7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape)  # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH5FG_6pfYtF",
        "outputId": "d2d315cc-2e4a-4c3b-ff04-d758ce7259a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUxAWnRJfZBJ",
        "outputId": "5dee7b4b-db5b-402e-d0b3-669d551570cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "3NZaKmtFfZ1z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X)\n",
        "  loss = criterion(outputs.view(-1, input_size), Y.view(-1))  # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "  loss.backward() # 기울기 계산\n",
        "  optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "  # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "  result = outputs.data.numpy().argmax(axis=2)  # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "  result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "  print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F63pHDnpfaFo",
        "outputId": "0820dbfd-fbd1-4064-8580-4a5592da99e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.6503998041152954 prediction:  [[2 2 2 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  eeeee\n",
            "1 loss:  1.3086830377578735 prediction:  [[3 3 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  llle!\n",
            "2 loss:  1.0701619386672974 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "3 loss:  0.8493860363960266 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "4 loss:  0.6647893190383911 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "5 loss:  0.531540036201477 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "6 loss:  0.4299473166465759 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "7 loss:  0.33964022994041443 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "8 loss:  0.2567402422428131 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.18563823401927948 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.1283232718706131 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.08494285494089127 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.05588988587260246 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.038542240858078 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.028137212619185448 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.021588478237390518 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.0172199048101902 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.01406843215227127 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.011580446735024452 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.00945248268544674 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.007551476359367371 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.005878431722521782 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.004511903040111065 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.0035025831311941147 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.0028095238376408815 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.0023405810352414846 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.0020129855256527662 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.0017728159436956048 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.0015886699547991157 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0014425694243982434 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.0013234916841611266 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0012245950056239963 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.001141222775913775 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0010701434221118689 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0010088139679282904 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0009556429577060044 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0009091083775274456 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0008679978782311082 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0008316457970067859 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0007992436876520514 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0007702208240516484 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0007440774352289736 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0007205045549198985 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0006991216214373708 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0006797142559662461 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0006619732594117522 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0006456844275817275 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0006307287258096039 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0006169634871184826 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0006042458117008209 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0005923376302234828 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0005814057658426464 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0005711644189432263 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0005616374546661973 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0005526818567886949 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0005442501860670745 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0005363662494346499 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.000528863281942904 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0005218366859480739 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0005152149242348969 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0005088788457214832 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0005028524319641292 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0004970640875399113 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0004916330799460411 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0004863926151301712 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0004814140556845814 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0004766260681208223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0004719332791864872 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.00046750251203775406 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0004631908086594194 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.00045906967716291547 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.00045506766764447093 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.000451160769443959 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0004474445595405996 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.00044375209836289287 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.00044022640213370323 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0004367721267044544 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.000433389242971316 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0004301255103200674 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.00042698089964687824 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.00042378855869174004 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.000420786818722263 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0004177850787527859 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0004148309235461056 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0004119958612136543 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0004091608279850334 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.00040642108069732785 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.00040368124609813094 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0004010844568256289 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.00039841607213020325 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.000395890703657642 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0003933176049031317 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.00039083982119336724 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.00038840965135023 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0003859556163661182 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0003835969546344131 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.00038126209983602166 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.00037892715772613883 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0003766160807572305 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0003743764536920935 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 더 많은 데이터로 학습한 문자 단위 RNN(Char RNN)\n"
      ],
      "metadata": {
        "id": "pl69KgUDfb9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] 필요 도구 임포트"
      ],
      "metadata": {
        "id": "nnigsMnpfgz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "-ML1NwunfaT8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] 훈련 데이터 전처리하기"
      ],
      "metadata": {
        "id": "OHy3w_-MfjoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up preple together to\"\n",
        "  \"collect wood and don't assign them tasks and work, but rather\"\n",
        "  \"teach them to long for the endless immensity of the sea.\")"
      ],
      "metadata": {
        "id": "CTQ5a9G6flc-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence))  # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"
      ],
      "metadata": {
        "id": "MuRIhBiYfljv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(char_dic) # 공백도 여기서는 하나의 원소"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PZFeW70fmT3",
        "outputId": "b60fac06-dd82-41e5-f7fe-535bb78eaf7d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'u': 0, 'h': 1, 'n': 2, 'k': 3, 'l': 4, 'e': 5, 'w': 6, \"'\": 7, 'f': 8, '.': 9, 'p': 10, 'o': 11, 'm': 12, 'y': 13, ' ': 14, 't': 15, 'a': 16, ',': 17, 'b': 18, 'r': 19, 's': 20, 'c': 21, 'g': 22, 'i': 23, 'd': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기: {}'.format(dic_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUXvvAt5fmhe",
        "outputId": "f3a385e6-0019-42d5-97a9-ae6e64c423ba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10   # 임의 숫자 지정\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "AZ9Z94SMfnqp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length) :\n",
        "  x_str = sentence[i:i + sequence_length]\n",
        "  y_str = sentence[i +1: i + sequence_length + 1]\n",
        "  print( i, x_str, '->', y_str)\n",
        "\n",
        "  x_data.append([char_dic[c]for c in x_str])  # x str to index\n",
        "  y_data.append([char_dic[c] for c in y_str]) # y str to index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UV60uGufnt-",
        "outputId": "417fa823-06e1-4237-8dad-4224b94cbd88"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pr\n",
            "35 drum up pr -> rum up pre\n",
            "36 rum up pre -> um up prep\n",
            "37 um up prep -> m up prepl\n",
            "38 m up prepl ->  up preple\n",
            "39  up preple -> up preple \n",
            "40 up preple  -> p preple t\n",
            "41 p preple t ->  preple to\n",
            "42  preple to -> preple tog\n",
            "43 preple tog -> reple toge\n",
            "44 reple toge -> eple toget\n",
            "45 eple toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether toc\n",
            "52 gether toc -> ether toco\n",
            "53 ether toco -> ther tocol\n",
            "54 ther tocol -> her tocoll\n",
            "55 her tocoll -> er tocolle\n",
            "56 er tocolle -> r tocollec\n",
            "57 r tocollec ->  tocollect\n",
            "58  tocollect -> tocollect \n",
            "59 tocollect  -> ocollect w\n",
            "60 ocollect w -> collect wo\n",
            "61 collect wo -> ollect woo\n",
            "62 ollect woo -> llect wood\n",
            "63 llect wood -> lect wood \n",
            "64 lect wood  -> ect wood a\n",
            "65 ect wood a -> ct wood an\n",
            "66 ct wood an -> t wood and\n",
            "67 t wood and ->  wood and \n",
            "68  wood and  -> wood and d\n",
            "69 wood and d -> ood and do\n",
            "70 ood and do -> od and don\n",
            "71 od and don -> d and don'\n",
            "72 d and don' ->  and don't\n",
            "73  and don't -> and don't \n",
            "74 and don't  -> nd don't a\n",
            "75 nd don't a -> d don't as\n",
            "76 d don't as ->  don't ass\n",
            "77  don't ass -> don't assi\n",
            "78 don't assi -> on't assig\n",
            "79 on't assig -> n't assign\n",
            "80 n't assign -> 't assign \n",
            "81 't assign  -> t assign t\n",
            "82 t assign t ->  assign th\n",
            "83  assign th -> assign the\n",
            "84 assign the -> ssign them\n",
            "85 ssign them -> sign them \n",
            "86 sign them  -> ign them t\n",
            "87 ign them t -> gn them ta\n",
            "88 gn them ta -> n them tas\n",
            "89 n them tas ->  them task\n",
            "90  them task -> them tasks\n",
            "91 them tasks -> hem tasks \n",
            "92 hem tasks  -> em tasks a\n",
            "93 em tasks a -> m tasks an\n",
            "94 m tasks an ->  tasks and\n",
            "95  tasks and -> tasks and \n",
            "96 tasks and  -> asks and w\n",
            "97 asks and w -> sks and wo\n",
            "98 sks and wo -> ks and wor\n",
            "99 ks and wor -> s and work\n",
            "100 s and work ->  and work,\n",
            "101  and work, -> and work, \n",
            "102 and work,  -> nd work, b\n",
            "103 nd work, b -> d work, bu\n",
            "104 d work, bu ->  work, but\n",
            "105  work, but -> work, but \n",
            "106 work, but  -> ork, but r\n",
            "107 ork, but r -> rk, but ra\n",
            "108 rk, but ra -> k, but rat\n",
            "109 k, but rat -> , but rath\n",
            "110 , but rath ->  but rathe\n",
            "111  but rathe -> but rather\n",
            "112 but rather -> ut rathert\n",
            "113 ut rathert -> t ratherte\n",
            "114 t ratherte ->  rathertea\n",
            "115  rathertea -> ratherteac\n",
            "116 ratherteac -> atherteach\n",
            "117 atherteach -> therteach \n",
            "118 therteach  -> herteach t\n",
            "119 herteach t -> erteach th\n",
            "120 erteach th -> rteach the\n",
            "121 rteach the -> teach them\n",
            "122 teach them -> each them \n",
            "123 each them  -> ach them t\n",
            "124 ach them t -> ch them to\n",
            "125 ch them to -> h them to \n",
            "126 h them to  ->  them to l\n",
            "127  them to l -> them to lo\n",
            "128 them to lo -> hem to lon\n",
            "129 hem to lon -> em to long\n",
            "130 em to long -> m to long \n",
            "131 m to long  ->  to long f\n",
            "132  to long f -> to long fo\n",
            "133 to long fo -> o long for\n",
            "134 o long for ->  long for \n",
            "135  long for  -> long for t\n",
            "136 long for t -> ong for th\n",
            "137 ong for th -> ng for the\n",
            "138 ng for the -> g for the \n",
            "139 g for the  ->  for the e\n",
            "140  for the e -> for the en\n",
            "141 for the en -> or the end\n",
            "142 or the end -> r the endl\n",
            "143 r the endl ->  the endle\n",
            "144  the endle -> the endles\n",
            "145 the endles -> he endless\n",
            "146 he endless -> e endless \n",
            "147 e endless  ->  endless i\n",
            "148  endless i -> endless im\n",
            "149 endless im -> ndless imm\n",
            "150 ndless imm -> dless imme\n",
            "151 dless imme -> less immen\n",
            "152 less immen -> ess immens\n",
            "153 ess immens -> ss immensi\n",
            "154 ss immensi -> s immensit\n",
            "155 s immensit ->  immensity\n",
            "156  immensity -> immensity \n",
            "157 immensity  -> mmensity o\n",
            "158 mmensity o -> mensity of\n",
            "159 mensity of -> ensity of \n",
            "160 ensity of  -> nsity of t\n",
            "161 nsity of t -> sity of th\n",
            "162 sity of th -> ity of the\n",
            "163 ity of the -> ty of the \n",
            "164 ty of the  -> y of the s\n",
            "165 y of the s ->  of the se\n",
            "166  of the se -> of the sea\n",
            "167 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExtchxXzfnzN",
        "outputId": "92cdafe1-b845-4c79-8366-61f457f72642"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23, 8, 14, 13, 11, 0, 14, 6, 16, 2]\n",
            "[8, 14, 13, 11, 0, 14, 6, 16, 2, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]  # x데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "T_cNul4ffpSx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기: {}'.format(X.shape))\n",
        "print('레이블의 크기: {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC6SMi8dfqdQ",
        "outputId": "e7f831cc-ed64-4142-efd9-3fa833416de2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기: torch.Size([168, 10, 25])\n",
            "레이블의 크기: torch.Size([168, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO6_CPY3frPT",
        "outputId": "ff62b792-fd58-4f8d-b56f-d5b4d23f2e50"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY1VTrZIfrX3",
        "outputId": "02701d06-de89-460f-e2d0-ebe075b08b2b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 8, 14, 13, 11,  0, 14,  6, 16,  2, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3] 모델 구현하기"
      ],
      "metadata": {
        "id": "VMIyUbdSfsK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):   # 현재 hidden_size는 dic_size와 같음.\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers = layers, batch_first = True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "DGGqVVRtfvf5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
      ],
      "metadata": {
        "id": "CstWNZTJfvkE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cirterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "Si-0QaBNfvof"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = net(X)\n",
        "print(outputs.shape)  # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poPuNDQ5fvs7",
        "outputId": "60d467ad-9f44-4789-d511-e5727d8a311e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, dic_size).shape) #2차원 텐서로 변환."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNkziPFAfvzv",
        "outputId": "f5602ed0-a088-4898-f540-c2b9b4dfe228"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-tYBCnnfv4q",
        "outputId": "bba255df-9e4d-4541-8301-7981dd3d1ab1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([168, 10])\n",
            "torch.Size([1680])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X)  # (170, 10, 25) 크기를 가진 텐서르 매 에포크마다 모델의 입력으로 사용\n",
        "  loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # results의 텐서 크기는 (170,10)\n",
        "  results = outputs.argmax(dim=2)\n",
        "  predict_str = \"\"\n",
        "  for j, result in enumerate(results):\n",
        "    if j == 0:  # 처음에는 예측 결과를 전부 가져오지만\n",
        "      predict_str += ''.join([char_set[t] for t in result])\n",
        "    else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "     predict_str += char_set[result[-1]]\n",
        "  print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i4e2pY5fwwY",
        "outputId": "fa20b83f-e8eb-478f-ecdd-7493a90c922a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
            "ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
            "eee ee  e  e e eeeeee eeee ee eee e eeeeeee ee ee  eeeeeeee eeeeeeeee eeeeee eeeeeee eeeee eeeeeeeeee  ee eeeeee eeeee eeeeeeeee eeeeee eee eeeeeeee ee eee eeeeeeeeee ee eee eee\n",
            " rbo ..o .  rfr .fr .rrr .rnrr ..ffr r rf.t ..  ff ffhfff frrfrn.f frrffrrf.  r.r rfr  nnf. frffr. h.f ffrrff f.r rrr.. . f rff.rn. rr.rh..  rf  rf.  ff r.f rf  .nffrrrnrh..  fr\n",
            "tot tt  tt t tt pt pp t ttoit t t t tp t  toit t tu pptptpp tt it tup tp t tut tpt t tptp t pt t tt tp t t tpt t t t tt t t tputott tottott t tpt tttuttt ptt tpt pp tt cp pt t i\n",
            "to o t  ttoto toio tt to to tott ttontoto to t to o ttitt to toio toto toto otottt tott at o t to t itototonio toio n t to t toto t to to t to t  nt nt ntitto t ttioitoctoit tt \n",
            "ao ontotod dodtoe d t t   n ton   i  t d  to t   t n t  t t dt d    d n ntododod t t d    t et to t itododon t a e a    t d  t d et aodtodt dod t eto a n i i n t a   t  a  t  i \n",
            "iotontit s eontht a a a e h pon   in ahdonao a  nshn    s dhnl e n  aon  t e don  nin  i  ahel thnl  ihd aon   i a a nl t d nl dhel shnlon ntod a nl    h a  nn l e heao ah l   s\n",
            "ght n shnl eh ph ga b e ein bo ss eo a eh ph as ethel  el dh a  s   wonnst d t eas ea  ih ph a eh as ih ls n s th  eh a a s  a th a ih ph a inn s  lthel  as    s  gh pn bh l  es\n",
            "g,t e tonl t ephe d w t egn po es eo a th p  e  et eet el whel  e   toneet d d ees eoe gn phel th as tod ton s t t eoe el    s them ahepo e tor t elt el  s ion s  gh pnewhelt e \n",
            "g,tor tonr toet d d d doe m toeet doem to poem  etoe t e  toer ee d toreet d doeet doe toet e  toems tododonee tot tot e   t dethe  to moet tod toe toe e t tonee  gotaoepoert e \n",
            "lmtonetmnr to thtld totoeto tonts do m ta toem ertoeet er to m d  r toreetod to ts doettnrtoer to ts tnr toned tot to mer tr detoem to to t toretoert r e tttonee tto to toertet \n",
            "lmtonttmnr to thild toto to tontt do m ta toem ertone  er to m d  t tore tnd to tt dr ttn toem to ts tnd toned tat da mer tt d toem to to t tonetoemt t e tttmn ettt, to toemttt \n",
            "lmtonetanr to t tld t tsel, ton't dr m da toem e to e  er to m e  d tonn tnd tonet dr ttn toem to ts tnd tored tatoda  er tt d them to to t tor toem et e tstmn n  t, to toemtet \n",
            "lmtor tona to puild tnthil, ton't drum da do m e th    er to m ee t tonr tnd don't drsstn them to ts tnd dored tutoda  er essd them to phn' tor thems t e tstmd n it, to phemset \n",
            "lmwon tona to phild dnthil, ton't drum dp doem e th ether th m ee a tond tnd don t drsian them to as tnd dor , but dat er essh ther to aon  tor themant n tsimd nsit, po phemses \n",
            "lmwon tona to auild dntoil, don t doum tp doem e th ether to olee a tore and don t drsian them to as and don , but dat ertesch ther to aonk tor thertnt d tsimuensiay po thertes \n",
            "lmton tona to auild dntoil, don't doum tp poem e th ether th o  eca torh and donkt drsian ther to ts and don , but dat er esch ther to aonk tor thersnt e ssimmensiay po therses \n",
            "lmton tana to auild d dhi , don't doum ta boem e thgether th o  ect dorh tnd don't drsian ther to ss and don , but gat ertesch ther to ao e ton thersntle siimmensiay oo therses \n",
            "lmronetant to tuild d dhip, don't drum ta boem e together to ol ect dord and don't drsian ther to ss and donk, but gat ertetch ther to lo   tor toertndle siimmensity oo thertes \n",
            "lmron tant to tuild b dhip, don't drum ga poemle together to ollect wood and don't drsign ther toess and donk, but gathertetch ther to long tor toertnd e ssimmensity oo thertes \n",
            "lmron tant to tuild b dhip, don't drum da poemle togethem tocollect wond and don't drsign them toess and work, but eather etch them to long tor themtndle ssimmensity oo themses \n",
            "lmdon tant to luild b dhip, don't urum dp poemle togethem th ollect wond and won't ussign them toeks and bork, but eather each them to lon' tor themtndle seimmensity oo themses \n",
            "lmdon tant to luild b dhip, don't urum dp poemle tocether th ollect wool and won't ussign ther tosks and work, but datherteach them to lon' tor thertnd e seimmensipy oo themtesc\n",
            "lmdor tant to luild b thip, don't urum dp premle tocether tocollect wood and won't dssign ther tosks and work, but dather each ther to lon' tor thertndle s immensipy of themsesc\n",
            "lmror tant to luild a thip, don't arum tp preple together tocollect wood and don't dssign ther tosks and dork, but datherteach them to long for themsndless immensity of the siss\n",
            "lmror tant to luild a thip, don't arum up preple togethem tocollect wood and don't assign them tosks and dork, aut datheraeach them to long for themsndless immensity of thensiat\n",
            "lmror tant to build a thip, don't arum dp preple togethem tocollect wood and don't assign them tosks and dork, aut dather each them to long for themsndless immensity of themsiac\n",
            "lmror tant to build a thip, don't arum dp pfeple togethem to ollect wood and don't assign them tosks and dork, but dather each them to bong for themsndless immensity of themseac\n",
            "gmror tant to build a thip, don't arum up pfeple togethem tocollect wood and don't assign them tosks and work, but dather each them to long for the sndless immensity of themseac\n",
            "gmdor tant to build a thip, don't arum up preple togethem tocollect wood and don't assign ther tosks and work, but dather each them to long for the sndless immensity of the seat\n",
            "amdor want to build a thip, don't arum up preple together tocollect wood and don't assign them tosks and work, but ratherteach them to long for the tndless immensity of the teat\n",
            "amyor want to luild a ship, don't arum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the sndless immensity of themseat\n",
            "gmyor want to luild a ship, don't arum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the sndless immensity of themseat\n",
            "gmyor want to build a ship, don't arum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the sndless immensity of themseat\n",
            "goyor want to build a ship, don't arum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the sndless immensity of the seat\n",
            "aoyou want to build a ship, don't arum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the tndless immensity of the seac\n",
            "toyou want to build a ship, don't arum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the tndless immensity of the seac\n",
            "toyou want to build a ship, don't arum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the sndless immensity of the seac\n",
            "toyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "goyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "goyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach ther to long for the endless immensity of the seas\n",
            "goyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach ther to long for the endless immensity of the seas\n",
            "goyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the eeat\n",
            "goyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the eeat\n",
            "goyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the eeat\n",
            "goyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the eeae\n",
            "goyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the eeae\n",
            "goyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the eeat\n",
            "foyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the eeat\n",
            "foyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "foyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "toyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seae\n",
            "toyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the eeat\n",
            "toyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seae\n",
            "toyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seae\n",
            "loyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "loyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "loyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seae\n",
            "loyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seae\n",
            "loyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seae\n",
            "loyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them ta long for the endless immensity of the seae\n",
            "loyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "loyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "loyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "moyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "moyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "moyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seae\n",
            "moyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "moyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "foyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "foyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the seat\n",
            "foyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "foyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "foyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "tnyou want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up preple together tocollect wood and don't assign them tasks and work, but ratherteach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    }
  ]
}